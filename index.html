<!DOCTYPE HTML>

<html>

<head>
  <title>MMVR</title>

  <link rel="stylesheet" href="template.css">
</head>

<body>
  <br>

  <center><span style="font-size: 44px; font-weight: bold;">Multi-modal Robustness Analysis Against Language and Visual Perturbations</span></center><br/>

  <table align=center width=600px>
    <tr>
      <td align=center width=150px>
        <center>
          <span style="font-size: 22px">
            <a href="." target="_top">[Home]</a>
          </span>
        </center>
      </td>
      
<!--       <td align=center width=150px>
        <center>
          <span style="font-size: 22px">
            <a href="." target="_top">[Paper]</a>
          </span>
        </center>
      </td> -->

      <td align=center width=150px>
        <center>
          <span style="font-size: 22px">
            <a href="./dataset.html" target="_top">[Dataset]</a>
          </span>
        </center>
      </td>
      
<!--       <td align=center width=150px>
        <center>
          <span style="font-size: 22px">
            <a href="./code/code.zip" target="_top">[Code]</a>
          </span>
        </center>
      </td> -->

      <td align=center width=150px>
        <center>
          <span style="font-size: 22px">
            <a href="./examples.html" target="_top">[Examples]</a>
          </span>
        </center>
      </td>
      
    </tr>
  </table>

  <br>
  <br>


  <table align=center width=800px>
    <center><img src="./images/PerturbationTypesCombined.png" width=800px /></center> <br>
    <center>Different real-world perturbations used in this study. </center>
  </table>

  <br>

  <br>

  <center>
    <h2>Abstract</h2>
  </center>
  <div style="width: 750px; margin: 0 auto; text-align:center; text-align: justify; text-justify: inter-ideograph;">
 <p>Joint visual and language modeling on large-scale datasets has recently shown a good progress in multi-modal tasks
   when compared to single modal learning. However, robustness of these approaches against
   real-world perturbations has not been studied. In this work, we perform the first extensive robustness study of such
   models against various real-world perturbations focusing on video and language. We focus on text-to-video retrieval
   and propose two large-scale benchmark datasets, <i>MSRVTT-P</i> and <i>YouCook2-P</i>, which utilize 90 different visual and 35 different textual perturbations. The study reveals some interesting findings:
   1) The studied models are more robust when text is perturbed versus when video is perturbed 
 2) The transformer text encoder is more robust on non-semantic changing text perturbations and visual perturbations compared to word embedding approaches.
 3) Using two-branch encoders in isolation is typically more robust than when architectures use cross-attention.
 We hope this study will serve as a benchmark and guide future research in robust multimodal learning.</p>
  </div>
   <br>
  <br>
<hr> <br>
  <center>
    <h2>Sample perturbations</h2>
    <center>Severity increasing from left to right.</center>
  </center><br>
  <table align=center width=800px>
    <center>
      <img src="./images/boxjumble_vido1_sev1.gif" width=150px />
      <img src="./images/boxjumble_vido1_sev2.gif" width=150px />
      <img src="./images/boxjumble_vido1_sev3.gif" width=150px />
      <img src="./images/boxjumble_vido1_sev4.gif" width=150px />
      <img src="./images/boxjumble_vido1_sev5.gif" width=150px />
    </center>
    <center>Box jumbling </center><br>
    <center>
      <img src="./images/rotate_vido3_sev1.gif" width=150px />
      <img src="./images/rotate_vido3_sev2.gif" width=150px />
      <img src="./images/rotate_vido3_sev3.gif" width=150px />
      <img src="./images/rotate_vido3_sev4.gif" width=150px />
      <img src="./images/rotate_vido3_sev5.gif" width=150px />      
    </center>
    <center>Random rotation</center><br>
    <center>
      <img src="./images/motionblur_vido2_sev1.gif" width=150px />
      <img src="./images/motionblur_vido2_sev2.gif" width=150px />
      <img src="./images/motionblur_vido2_sev3.gif" width=150px />
      <img src="./images/motionblur_vido2_sev4.gif" width=150px />
      <img src="./images/motionblur_vido2_sev5.gif" width=150px />        
    </center>
    <center>Motion blur</center><br>
    
  </table>
  <br><br> <hr><br>
  <center>
    <h2>Robustness analysis</h2>
  </center>
  <table align=center width=800px>
    <center><img src="./images/TeaserAll.png" width=600px /></center><br>
    <center>A performance and robustness visualization of multimodal models on YouCook2-P and MSRVTT-P. 
      y-axis: relative robustness (lower is better), x-axis: R@5 on text-to-video retrieval,  
      , and the size of circle indicates FLOPs. The models used are 
      <a href="https://github.com/facebookresearch/fairseq/tree/main/examples/MMPT">Videoclip</a>,
      <a href="https://github.com/microsoft/UniVL">Univl</a>,
      <a href="https://github.com/gingsi/coot-videotext">COOT</a>, and
      <a href="https://github.com/antoine77340/MIL-NCE_HowTo100M">HowTo100M MIL</a>.
      The results vary based on the dataset, 
      for example, 
<a href="https://github.com/facebookresearch/fairseq/tree/main/examples/MMPT">Videoclip</a> from scratch is both more robust and a better performer on 
      the MSRVTT dataset than on YouCook2. This relationship <i>indicatea a difference on
      how models <b>handle clips from long, complex activities</b> compared to videos that 
      are short and of a simple activity</i>.
      The top performers are consistently <a href="https://github.com/facebookresearch/fairseq/tree/main/examples/MMPT">Videoclip</a>
      and <a href="https://github.com/microsoft/UniVL">Univl</a>, which are also the larger 
      models. </center>
  </table>

 

  <br>

</body>

</html>
